---
title: "Computational Musicology"
author: "Amber Kusters"
date: '2023-03-12'
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme:
      bg: "#fffffa"
      fg: "#1c1b1b" 
      primary: "#6190AF"
      navbar-bg: "#6190AF"
      base_font: 
        google: Source Sans Pro
      heading_font:
        google: Sen
---
```{r, eval = FALSE, echo=FALSE}
remotes::install_github('jaburgoyne/compmus')
```

```{r libraries, message=FALSE, echo=FALSE}
library(tidyverse)
library(spotifyr)
library(ggplot2)
library(plotly)
library(shiny)
library(knitr)
library(grid)
library(compmus)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Week 11: Timbre coefficients in sad playlists

```{r, echo = FALSE}
sadbitchhours <-
  get_playlist_audio_features(
    "thesoundsofspotify",
    "3h7anos6b21Kvv7N5Sj9v6"
  ) |>
  slice(1:30) |>
  add_audio_analysis()
lifesucks <-
  get_playlist_audio_features(
    "thesoundsofspotify",
    "37i9dQZF1DX3YSRoSdA634"
  ) |>
  slice(1:30) |>
  add_audio_analysis()
sadplaylists <-
  sadbitchhours |>
  mutate(genre = "Sad *itch Hour") |>
  bind_rows(lifesucks |> mutate(genre = "Life Sucks"))

sadplaylists |>
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) |>
  select(genre, timbre) |>
  compmus_gather_timbre() |>
  ggplot(aes(x = basis, y = value, fill = genre)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Playlist")
```

***
This week assignment is this page and the next!

The graph on the left compares the two sad playlists by looking at the timbre coefficients that Spotify uses. On most of these coefficients, it looks like the playlist are fairly similar, only deviating a little bit. However, when looking at the second timbre coefficient, it comes to my attention that the Life Sucks playlist, created by Spotify has a bigger range of brightness. The documentation that explains a part of the timbre coefficients refers to this second coefficient as brightness. This would mean that the Life Sucks playlist has more songs that are interpreted as brighter than the Sad Bitch Hour playlist. This actually also confirms that the playlist created by Spotify contains more happier songs than the one created by my friend, which was also displayed in the histogram comparing which playlist is more sad.

### Week 11: Consistent keygrams {data-commentary-width=500}
```{r templates, echo = FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3))
```

```{r keygram, echo = FALSE}
iwouldstay <-
  get_tidy_audio_analysis("3GZFKiGVYv3SBQ6PLf3JgF") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

keygramkrezip <- iwouldstay |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", title = "Keygram of I Would Stay by Krezip")
keygramkrezip
```

***

It stood out that when looking at different keygrams for sad music, there isn't much variation in the key throughout the song. This graphs gives an example, showing that the majority is in D major key. It is a keygram of I Would Stay by Krezip, which is a very famous sad song in the Netherlands. In my final portfolio I will try to show multiple keygrams from the sad playlist to give a more complete view of the consistancy in key throughout the songs.

<iframe src="https://open.spotify.com/embed/track/3GZFKiGVYv3SBQ6PLf3JgF" width="100%" height="85" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

### Corpus
When you are feeling down it is comforting to listen to sad music as it matches your mood. Sometimes the music can make you feel less alone or it feels like someone else relates to your problems. The corpus I chose is a playlist full of sad songs. A very good playlist of a friend of mine called "Sad *itch Hours" is a playlist with 203 sad songs. I find it interesting that there are also different playlists by Spotify that focus on sad music, but also differentiate in, for example, heartbreak or just having a bad day. I am interested to learn what makes a playlist a sad playlist? To do this, I choose to include the Spotify playlist "Life Sucks" in my corpus, which has 200 sad songs. In contrast to this I would also like to add playlists focussed on happy moods to my corpus, to look at the differences between happy and sad music. This will include "Happy Favorites" and "Wake Up Happy", which are both playlists made by Spotify and containing 150 songs each. All playlist included in this corpus contain songs of various genres and artists, not focussing on one specific genre.

The natural groups I would like to investigate in the sad playlist is artists and valence of songs. It is interesting to see which artist has the most songs in the sad playlists and whether an artist might be more diverse by having both happy and sad songs. Futhermore, focussing on the difference between the spotify generated playlist and the one of my friend, to see if there is anything that stands out. Additionally, I would like to see whether there are factors that influence valence, for example, danceability or speechiness, and to compare this with happy and sad music.

I believe the tracks in my corpus are very representative as the playlist "Sad *itch Hours" has a very diverse group of artists and therefore also genres. However, it does miss genres like country or metal sad music, as most songs are slow hip-hop, r&b or pop songs. The playlist generated by Spotify of sad and happy music are also very broad, but do focus on music that is more recent, and don't include many old songs. I chose playlists that are not focussed on one specific genre to make a better comparison.

Typical tracks in my corpus for sad songs are Don't Speak by No Doubt, songs by Billie Eilish, Lana del Rey, or Adele. Looking at the happy playlist by Spotify, it varies a lot from feel good songs to real party anthems.

### Which playlist is more sad? {data-commentary-width=600}

```{r load playlists, echo=FALSE}
sadplaylist <- get_playlist_audio_features("", "3h7anos6b21Kvv7N5Sj9v6")
sadspotify <- get_playlist_audio_features("", "37i9dQZF1DX3YSRoSdA634")
happyspotify <- get_playlist_audio_features("", "37i9dQZF1DWZKuerrwoAGz")
happyspotify2 <- get_playlist_audio_features("", "37i9dQZF1DX0UrRvztWcAU")
```

```{r combine datasets, echo=FALSE}
sadsongs <-
  bind_rows(
    sadplaylist |> mutate(category = "Sad *itch Hours Playlist"),
    sadspotify |> mutate(category = "Life Sucks Playlist")
  )
happysongs <-
  bind_rows(
    happyspotify |> mutate(category = "Happy Favorites"),
    happyspotify2 |> mutate(category = "Wake Up Happy")
  )

allsongs <-
  bind_rows(
    sadplaylist |> mutate(category = "Sad"),
    sadspotify |> mutate(category = "Sad"),
    happyspotify |> mutate(category = "Happy"),
    happyspotify2 |> mutate(category = "Happy")
  )

```

```{r histogram valence, echo=FALSE}
sad_histogram <-
  ggplot(sadsongs, aes(x = valence)) +
  geom_histogram(binwidth = 0.1,color="black", fill="lightblue") +
  facet_wrap(~category) + 
  theme_light() +
  labs(title="Distribution of valence in sad playlists",
    x = "Valence",
    y = "Count")
ggplotly(sad_histogram)
```
***
To start off I wanted to see what the level of valence is according to Spotify in the two sad playlists. I used the histogram to focus on only one feature. According to Spotify, valence describes the musical positiveness of a song from 0 to 1, meaning more negative and more positive respectively. This figure shows that the playlist made by Spotify ("Life Sucks") is slightly more positive than the one my friend made, as it contains more songs with a higher valence value.

* Sad *itch Hours contains 203 songs
* Life Sucks, generated by Spotify contains 200 songs

<iframe src="https://open.spotify.com/embed/playlist/37i9dQZF1DX3YSRoSdA634" width="50%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

<iframe src="https://open.spotify.com/embed/playlist/3h7anos6b21Kvv7N5Sj9v6" width="50%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>


### Happy playlist vs. Sad playlist

```{r, echo=FALSE}
happy_sad_scatter <-
  ggplot(allsongs, aes(x=valence, y=danceability, color=category, size=energy)) +
  geom_point(alpha=0.6) + theme_light() + 
  scale_color_manual(values = c("#FF9999", "lightblue")) +
  labs(title="Relation between valence, danceability and senergy in happy and sad songs", x="Valence", y="Danceability", size="Energy", colour="Category" )
ggplotly(happy_sad_scatter)
```
***

Then I wanted to look at the differences between the happy and sad playlists, and look at features that might influence valence. Therefore, this next visualisation includes valence, danceablity and energy. Danceability rates how suitible a song is to dance to (1.0 being very danceable) and energy represents the intensity or activity, which is calculated with loudness, timbre, dynamic range, etc. (1.0 being very energetic). In this visualisation it is seen that the distribution of valence is broad in both happy and sad playlists, but there is also a division showing that overall happy music has higher valence than sad music, which was also expected. It also shows that positive music has a higher danceability, and the energy of positive music is mostly above the 0.50. Whereas only the blue dots (sad songs) have an energy of 0.25.

### Another Love Chromagram {data-commentary-width=500}

```{r, echo=FALSE}
anotherlove <-
  get_tidy_audio_analysis("3JvKfv6T31zO0ini8iNItO") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

anotherlove_chroma <- anotherlove |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Chromagram of Another Love by Tom Odell") +
  theme_minimal() +
  scale_fill_viridis_c()
anotherlove_chroma
```

***

This chromagram resembles the song Another Love by Tom Odell. A song that is more than 10 years old, but gained popularity in 2022 due to TikTok. It is a song that is part of the sad songs corpus. What I find interesting about this chromagram is how it starts off with more indiviual notes, but around 100 seconds in, there is a change. This is when the choir starts singing in the song and the tempo speeds up. In the chromagram it looks like the notes are more blurry and blend into each other. 

<iframe src="https://open.spotify.com/embed/track/3JvKfv6T31zO0ini8iNItO" width="100%" height="85" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

### Patterns in the saddest song of the corpus {data-commentary-width=500}
```{r week 9, echo=FALSE}
maria <-
  get_tidy_audio_analysis("0u4rkpmNtgcFxYHepnVF4v") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  maria |>
    compmus_self_similarity(pitches, "aitchison") |>
    mutate(d = d / max(d), type = "Chroma"),
  maria |>
    compmus_self_similarity(timbre, "euclidean") |>
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title="Carry You by Novo Amor")
```

***
The new addition for week 9. On the left you can see two self-similarity matrices for Carry You by Novo Amor. This song is the saddest song in the corpus, with a valence of 0.035, originated from the Life Sucks playlist of Spotify. When listening to the song for the first time it feels like it is pretty consistent throughout, but the matrices do show some patterns. Looking at the chroma self-similarity matrix, it shows that the first 90 seconds are rather similar, but the timbre matrix already shows a change around 40 seconds. The artist starts singing around this time, so that explains the change in timbre. From 90 seconds on wards, new background instruments are added and therefore both matrices form a new block. This continues until the cross at 180 seconds. This change in timbre and chroma is caused by the almost silence as the music decreases and abruptly starts again after a few seconds. Overall the piece is quite repetitive, but the minor changes make it more interesting to listen to.

<iframe src="https://open.spotify.com/embed/track/0u4rkpmNtgcFxYHepnVF4v" width="100%" height="85" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

